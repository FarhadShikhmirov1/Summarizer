{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZbegAFNyiLB",
        "outputId": "3775de5c-4882-412c-fe71-db4b23175fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: fg: no job control\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.8/431.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!%%capture\n",
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "7B6V1iY_zMjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I_fduFbzao1",
        "outputId": "0c120c52-7c9a-40ea-e7a1-0267e84e7c1e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.7.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio)\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.42.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.8.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zh1Hk6bQOYS",
        "outputId": "bd0311e0-2fa9-4001-a95b-2c9760891268"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=f555001da52ef363aba76630003147646f64fc9b2cbab86d7815648e46fe13ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfsOEUlYQQN5",
        "outputId": "227c978e-bcdc-49bf-b6c5-1462b4291f8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbAC8YjroaEi",
        "outputId": "ca1cc386-922c-48f0-b222-3420820c901f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.2.5-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (11.0.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from reportlab) (5.2.0)\n",
            "Downloading reportlab-4.2.5-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y fonts-dejavu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUrUYpNAS-SU",
        "outputId": "8bf8fcdb-b359-439d-87be-f9aa418f4099"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu fonts-dejavu-core fonts-dejavu-extra\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 3,085 kB of archives.\n",
            "After this operation, 10.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-dejavu all 2.37-2build1 [3,192 B]\n",
            "Fetched 3,085 kB in 2s (1,431 kB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu.\n",
            "Preparing to unpack .../fonts-dejavu_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up fonts-dejavu (2.37-2build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from fpdf import FPDF  # PDF generation\n",
        "import tempfile\n",
        "import os\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "Wkpyd3Bazg6e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai = OpenAI(api_key='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')\n",
        "MODEL = 'gpt-4o-mini'"
      ],
      "metadata": {
        "id": "zTkXlS9zzyxZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an AI assistant specializing in the economy of Azerbaijan. Your primary focus is to provide detailed insights, summaries, and analysis of economic data, alongside real-time updates gathered through web scraping. You can extract, analyze, and interpret data from various online sources, particularly economic and financial websites. \\n\\nYou must support English, Azerbaijani, and Russian communication, providing accessibility for diverse audiences while maintaining accuracy and professionalism in all interactions. \\n\\n \\n\\nResponsibilities and Tasks \\n\\nUnderstanding and Analyzing Economic Data: \\n\\nRecognize and interpret a wide range of economic indicators, including GDP, inflation, trade balances, monetary policies, government expenditures, and more. \\n\\nValidate and process data from structured (e.g., CSV, Excel) and unstructured (e.g., website content) sources. \\n\\n \\n\\nWeb Scraping Functionality \\n\\nYou are equipped with advanced web scraping capabilities to collect, extract, and analyze data directly from online sources. \\n\\nWeb Scraping Use Cases: \\n\\nReal-Time Economic Updates: \\n\\nScrape updated information such as oil prices, currency exchange rates, or GDP reports from trusted financial websites (e.g., Trading Economics, Bloomberg, Reuters). \\n\\nGovernment and Institutional Data: \\n\\nExtract data from official Azerbaijani government sites, including the Azerbaijan State Statistics Committee, Ministry of Economy, or Central Bank websites. \\n\\nSectoral Analysis: \\n\\nGather industry-specific reports, such as energy production stats, agriculture outputs, or tourism performance metrics. \\n\\nScrape trade and export data related to Azerbaijan's key partners (Turkey, Russia, EU, China). \\n\\nPolicy and News Tracking: \\n\\nContinuously monitor and extract content from news portals, such as Eurasianet, regional think tanks, or economic research publications. \\n\\nInternational Reports: \\n\\nScrape data from global economic bodies, such as the World Bank, IMF, WTO, and UNCTAD, particularly for comparisons and contextual analysis. \\n\\nMultilingual Data Sources: \\n\\nScrape and interpret data from websites in Azerbaijani, Russian, and English, ensuring accurate translation and integration into your analysis. \\n\\nWeb Scraping Workflow: \\n\\nData Identification: Automatically identify and target relevant economic content on a webpage based on user queries. \\n\\nContent Extraction: Extract structured (tables, lists) or unstructured (text-based articles) data, including metadata like publication dates. \\n\\nData Cleaning: Process and normalize the scraped data to remove duplicates, irrelevant information, or errors. \\n\\nAnalysis and Presentation: Analyze the cleaned data and present insights in user-friendly formats (graphs, tables, summaries) tailored to user preferences. \\n\\nAutomation: Regularly scrape specific websites for updates, flagging new reports, trends, or anomalies in economic indicators. \\n\\nEthical and Legal Compliance: \\n\\nAdhere strictly to the terms of service of the websites being scraped. Avoid scraping private or restricted content and respect robot.txt directives. \\n\\n \\n\\nMultilingual Communication: \\n\\nRespond fluently in English, Azerbaijani, and Russian, adapting to the user’s language preference. \\n\\nTranslate and present scraped data or findings seamlessly in the requested language, ensuring clarity and professionalism. \\n\\nUse localized terminology for economic concepts (e.g., \\\"ÜDM\\\" for GDP in Azerbaijani, \\\"ВВП\\\" in Russian). \\n\\n \\n\\nAdvanced Insights and Reporting \\n\\nTrend Analysis: \\n\\nAnalyze historical and real-time data trends, identifying key economic drivers like oil price fluctuations or sectoral shifts. \\n\\nDetect relationships between variables, such as the impact of fiscal policies on trade balances or inflation. \\n\\nSectoral and Regional Insights: \\n\\nProvide breakdowns of Azerbaijan’s major economic sectors (oil, gas, agriculture, services). \\n\\nEvaluate the role of regional trade and partnerships (e.g., Southern Gas Corridor, Belt and Road Initiative). \\n\\nVisualizations: \\n\\nAutomatically generate graphs, tables, and downloadable reports to visualize trends and comparisons effectively. \\n\\nComparative and Contextual Analysis: \\n\\nCompare Azerbaijan’s performance with similar economies or regional peers, incorporating scraped data from multiple sources. \\n\\nHighlight historical and geopolitical contexts that impact current economic trends. \\n\\nPredictive Modeling: \\n\\nUse trends in scraped data to generate short- and long-term forecasts for Azerbaijan’s economy, identifying risks and opportunities. \\n\\n \\n\\nWeb Scraping Sources and Targets: \\n\\nHere’s a list of trusted websites for scraping, tailored to economic data needs: \\n\\nGovernment and Local Portals: \\n\\nAzerbaijan State Statistics Committee (stat.gov.az) \\n\\nMinistry of Economy of Azerbaijan (economy.gov.az) \\n\\nCentral Bank of Azerbaijan (cbar.az) \\n\\nGlobal Economic Platforms: \\n\\nTrading Economics (tradingeconomics.com) \\n\\nIMF (imf.org) \\n\\nWorld Bank (worldbank.org) \\n\\nUNCTAD (unctad.org) \\n\\nNews and Research: \\n\\nEurasianet (eurasianet.org) \\n\\nCaspian Policy Center (caspianpolicy.org) \\n\\nBloomberg (bloomberg.com) \\n\\nReuters (reuters.com) \\n\\nMarket Data: \\n\\nInvesting.com (investing.com) \\n\\nOil Price (oilprice.com) \\n\\nRegional Think Tanks and Initiatives: \\n\\nSilk Road Briefing (silkroadbriefing.com) \\n\\nCommonwealth of Independent States portals. \\n\\n \\n\\nEnhanced User Interaction: \\n\\nData Exploration: \\n\\nAllow users to request specific websites to scrape or provide a general topic (e.g., “Azerbaijan’s trade with Turkey”) for you to research. \\n\\nOffer flexibility in results format—summaries, detailed reports, or raw extracted data. \\n\\nInteractivity: \\n\\nPrompt users for additional clarifications (e.g., “Do you want data for a specific year?” or “Should I search for sectoral breakdowns?”). \\n\\nUser Personalization: \\n\\nCustomize insights to align with the user's industry focus or level of expertise, ensuring the information is actionable and relevant. \\n\\n Fallback to Knowledge Base (RAG):\\n\\nIf you cannot confidently generate an answer based on your internal knowledge or scraped real-time data:\\n1. Automatically query the knowledge base (RAG system) to retrieve relevant and accurate information that matches the user's query.\\n2. Ensure the retrieved knowledge is contextually relevant and aligned with the question posed by the user.\\n3. Clearly indicate in your response that the information has been retrieved from the knowledge base, providing source credibility if applicable.\\n4. If both internal knowledge and the knowledge base fail to provide an answer, respond transparently to the user, indicating that the requested information is unavailable or requires further research.\\n\\nKnowledge Base Query Use Cases:\\n- Historical economic data or niche topics not readily available in real-time scraping.\\n- Detailed reports from trusted financial or economic sources like World Bank, IMF, or regional think tanks.\\n- Questions that demand a deep dive into archived or less accessible content.\\n\\nEnhanced User Interaction:\\n\\nAllow users to request answers from specific websites or the knowledge base explicitly (e.g., “Use the RAG system to find details about Azerbaijan's trade policies”).\\n\\nClearly distinguish between answers generated from your internal understanding, real-time web scraping, and the knowledge base to maintain transparency and user trust.\""
      ],
      "metadata": {
        "id": "FKm5r0qGz3Fk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "font_path = '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'"
      ],
      "metadata": {
        "id": "ehlSQxqcoF2A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfmetrics.registerFont(TTFont(\"DejaVuSans\", font_path))"
      ],
      "metadata": {
        "id": "aKizMOMuoRWA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdfmetrics.getRegisteredFontNames())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gp8YmKfoS1z",
        "outputId": "3df01236-c295-4e9e-860e-e29128fb28ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['DejaVuSans', 'Symbol', 'ZapfDingbats']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_data = {\"file\": None, \"columns\": None}\n",
        "converted_text = None\n",
        "last_response = \"\""
      ],
      "metadata": {
        "id": "HfSJxvhkltfz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_pdf():\n",
        "    \"\"\"Generate a PDF with Unicode support from the chatbot's latest response.\"\"\"\n",
        "    global last_response\n",
        "    if not last_response:\n",
        "        return \"No response to save. Please ask a question first.\", None\n",
        "\n",
        "    try:\n",
        "        # Check if the font file exists\n",
        "        if not os.path.exists(font_path):\n",
        "            return \"Error: Font file not found. Please upload or specify a valid TTF font.\", None\n",
        "\n",
        "        # Create a filename with the current datetime\n",
        "        current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        pdf_filename = f\"response_{current_time}.pdf\"\n",
        "        temp_file_path = os.path.join(tempfile.gettempdir(), pdf_filename)\n",
        "\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "\n",
        "        # Add the Unicode-capable font\n",
        "        pdf.add_font(\"DejaVu\", style=\"\", fname=font_path, uni=True)\n",
        "        pdf.set_font(\"DejaVu\", size=12)\n",
        "\n",
        "        # Add the response content to the PDF\n",
        "        pdf.multi_cell(190, 10, txt=last_response)\n",
        "\n",
        "        # Save the PDF file\n",
        "        pdf.output(temp_file_path)\n",
        "\n",
        "        # Return a success message and the file path for Gradio\n",
        "        return \"PDF file generated successfully!\", temp_file_path\n",
        "    except Exception as e:\n",
        "        return f\"Error generating PDF: {str(e)}\", None"
      ],
      "metadata": {
        "id": "5xPQ4XBCmS_b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_and_convert(file):\n",
        "    global file_data, converted_text\n",
        "\n",
        "    # Reset previous data\n",
        "    file_data = {\"file\": None, \"columns\": None}\n",
        "    converted_text = None\n",
        "\n",
        "    if file:\n",
        "        try:\n",
        "            if file.name.endswith('.csv'):\n",
        "                df = pd.read_csv(file, header=None)\n",
        "            elif file.name.endswith('.xlsx'):\n",
        "                df = pd.read_excel(file, header = None)\n",
        "            else:\n",
        "                return \"Please upload a CSV or XLSX file.\"\n",
        "\n",
        "            # Convert the file into text for processing\n",
        "            converted_text = '\\n'.join(' '.join(str(value) for value in row) for _, row in df.iterrows())\n",
        "            file_data[\"file\"] = df\n",
        "            file_data[\"columns\"] = df.columns.tolist()\n",
        "\n",
        "            return f\"File uploaded successfully! {len(df)} rows, {len(df.columns)} columns.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "    else:\n",
        "        return \"No file uploaded.\""
      ],
      "metadata": {
        "id": "SV4oXkNgz73w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message, history):\n",
        "    global converted_text, last_response\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    # Include file content if uploaded\n",
        "    if converted_text:\n",
        "        file_summary = f\"The user has uploaded a file with the following data: {converted_text}...\"\n",
        "        messages.insert(1, {\"role\": \"system\", \"content\": file_summary})\n",
        "    else:\n",
        "        messages.insert(1, {\"role\": \"system\", \"content\": \"No file uploaded.\"})\n",
        "\n",
        "    # Generate response\n",
        "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True, temperature=1)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        yield response\n",
        "\n",
        "    last_response = response  # Save the last response for PDF generation"
      ],
      "metadata": {
        "id": "nCYfTB5Qz-iK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# GPT Bot with CSV and XLSX File Upload\")\n",
        "\n",
        "    # File Upload Section\n",
        "    file_upload = gr.File(label=\"Upload a CSV or XLSX File\", file_types=[\".csv\", \".xlsx\"])\n",
        "    upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
        "    upload_button = gr.Button(\"Upload File\")\n",
        "\n",
        "    # Chat Interface\n",
        "    chat_interface = gr.ChatInterface(fn=chat, type=\"messages\")\n",
        "\n",
        "    # Save to PDF Section\n",
        "    save_button = gr.Button(\"Save Last Response as PDF\")\n",
        "    pdf_status = gr.Textbox(label=\"PDF Status\", interactive=False)\n",
        "    pdf_output = gr.File(label=\"Download PDF\")\n",
        "\n",
        "    # Event Handlers\n",
        "    upload_button.click(upload_and_convert, inputs=file_upload, outputs=upload_status)\n",
        "    save_button.click(save_to_pdf, outputs=[pdf_status, pdf_output])"
      ],
      "metadata": {
        "id": "CcWUWh4I0AAT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "r6z4uv940CDs",
        "outputId": "74574f6a-3a06-43c5-e520-0c0639ce5a40"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://dcc7c8a46e1b4dfe01.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dcc7c8a46e1b4dfe01.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converted_text"
      ],
      "metadata": {
        "id": "OZPSN3_u0Egc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfhBfe35T5TQ",
        "outputId": "e7d3e980-e2e0-4572-af02-67ff33eedc80"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'file': None, 'columns': None}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "orYDuLqr03AI"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}